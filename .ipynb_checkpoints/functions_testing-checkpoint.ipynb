{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a96743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/anaconda3/lib/python3.9/site-packages (10.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/anaconda3/lib/python3.9/site-packages (from pyarrow) (1.21.5)\n",
      "Requirement already satisfied: fastparquet in /opt/anaconda3/lib/python3.9/site-packages (2022.12.0)\n",
      "Requirement already satisfied: cramjam>=2.3 in /opt/anaconda3/lib/python3.9/site-packages (from fastparquet) (2.6.2)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.9/site-packages (from fastparquet) (2022.7.1)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /opt/anaconda3/lib/python3.9/site-packages (from fastparquet) (1.5.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.9/site-packages (from fastparquet) (21.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/anaconda3/lib/python3.9/site-packages (from fastparquet) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2022.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from packaging->fastparquet) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Requirement already satisfied: panda in /opt/anaconda3/lib/python3.9/site-packages (0.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from panda) (63.4.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.9/site-packages (from panda) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests->panda) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests->panda) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests->panda) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests->panda) (3.3)\n",
      "Requirement already satisfied: dask in /opt/anaconda3/lib/python3.9/site-packages (2022.12.1)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /opt/anaconda3/lib/python3.9/site-packages (from dask) (2.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from dask) (21.3)\n",
      "Requirement already satisfied: click>=7.0 in /opt/anaconda3/lib/python3.9/site-packages (from dask) (8.0.4)\n",
      "Requirement already satisfied: partd>=0.3.10 in /opt/anaconda3/lib/python3.9/site-packages (from dask) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from dask) (2022.7.1)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /opt/anaconda3/lib/python3.9/site-packages (from dask) (0.11.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/anaconda3/lib/python3.9/site-packages (from dask) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->dask) (3.0.9)\n",
      "Requirement already satisfied: locket in /opt/anaconda3/lib/python3.9/site-packages (from partd>=0.3.10->dask) (1.0.0)\n",
      "Requirement already satisfied: pyspark in /opt/anaconda3/lib/python3.9/site-packages (3.3.1)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/anaconda3/lib/python3.9/site-packages (from pyspark) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow\n",
    "!pip install fastparquet\n",
    "!pip install --upgrade panda\n",
    "!pip install --upgrade dask\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48639063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.csv as pv\n",
    "import fastparquet\n",
    "import dask.dataframe as dd\n",
    "import csv\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from pandas.io import parsers\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d14b16e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path.cwd()\n",
    "dataset = 'yelp_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78357c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_csv(json_file_path):\n",
    "    # Read the JSON file in chunks, to avoid MemoryError\n",
    "    # lines=True for the json file with one object per line\n",
    "    chunks = pd.read_json(json_file_path, lines=True, chunksize=100000)\n",
    "    filename = json_file_path.stem + '.csv'\n",
    "    for i, chunk in enumerate(chunks): # Found some unusual rows on reviews, \n",
    "        chunk.dropna().to_csv(filename,\n",
    "            header=(i==0), mode='a', index=True, index_label='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b67de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_if_exist(dataset_name):\n",
    "    cwd = Path.cwd()\n",
    "    json_name = dataset_name + '.json'\n",
    "    csv_name = dataset_name + '.csv'\n",
    "    json_file_path_user = cwd.joinpath('yelp_dataset', json_name)\n",
    "    if cwd.joinpath(csv_name).is_file()==False:\n",
    "        convert_json_to_csv(json_file_path_user)\n",
    "    else:\n",
    "        # Avoid duplicate appending to the same file\n",
    "        cwd.joinpath(csv_name).unlink() # Deletes existing file, replace with updated one\n",
    "        convert_json_to_csv(json_file_path_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa34caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['yelp_academic_dataset_user', 'yelp_academic_dataset_tip', 'yelp_academic_dataset_review', 'yelp_academic_dataset_checkin', 'yelp_academic_dataset_business']\n",
    "for dataset in files:\n",
    "    append_if_exist(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3daa6f",
   "metadata": {},
   "source": [
    "Convert to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9e8de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_if_exist('yelp_academic_dataset_review')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978596ad",
   "metadata": {},
   "source": [
    "Testing Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "522ba37a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [88]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreade\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75bbb001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9w/zmrty_2d1rzg6mqzyxfyzkq00000gn/T/ipykernel_64267/726718714.py:4: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv('yelp_academic_dataset_review.csv', on_bad_lines='warn')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275.9857189655304\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "df2 = pd.read_csv('yelp_academic_dataset_review.csv', on_bad_lines='warn')\n",
    "df2 = df2.dropna()\n",
    "df2['index'] = pd.to_numeric(df2['index'])\n",
    "df2.to_parquet('yelp_academic_dataset_review.parquet', compression='gzip')\n",
    "end = time.time()\n",
    "print(end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ce2c7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2923301</th>\n",
       "      <td>2923301</td>\n",
       "      <td>Gmd5aXXHFBMTaiDt7rf-Sg</td>\n",
       "      <td>rPN6v1sNLgoLq7sJXZMvUA</td>\n",
       "      <td>owrGcj5a7W0QnWA7HwJ2Ag</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>UPDATE: this store is no longer open 24/7.\\n\\n...</td>\n",
       "      <td>2015-06-27 03:01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2923302</th>\n",
       "      <td>2923302</td>\n",
       "      <td>VVNJLuayIj6SbhtOY3XIcA</td>\n",
       "      <td>DbNKK25oOzfHxyfBHlOaDg</td>\n",
       "      <td>-dwqsjGDBnzzv2qQYyIC3g</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I noticed this place on the way to work one mo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2923303</th>\n",
       "      <td>We were welcomed at the door. The staff here w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2923304</th>\n",
       "      <td>There were so many great menu choices and a lo...</td>\n",
       "      <td>2016-03-21 19:31:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2923305</th>\n",
       "      <td>2923303</td>\n",
       "      <td>P7g9LAj9XJlWSyP5KFqY0Q</td>\n",
       "      <td>ZQ-hVBie3ls9u951cJkgSQ</td>\n",
       "      <td>3nuDjIVfclE99gf4KBidKw</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It's a Tampa thing...\\n\\nI was at Westshore Pl...</td>\n",
       "      <td>2018-01-12 22:14:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Unnamed: 0  \\\n",
       "2923301                                            2923301   \n",
       "2923302                                            2923302   \n",
       "2923303  We were welcomed at the door. The staff here w...   \n",
       "2923304  There were so many great menu choices and a lo...   \n",
       "2923305                                            2923303   \n",
       "\n",
       "                      review_id                 user_id  \\\n",
       "2923301  Gmd5aXXHFBMTaiDt7rf-Sg  rPN6v1sNLgoLq7sJXZMvUA   \n",
       "2923302  VVNJLuayIj6SbhtOY3XIcA  DbNKK25oOzfHxyfBHlOaDg   \n",
       "2923303                     NaN                     NaN   \n",
       "2923304     2016-03-21 19:31:55                     NaN   \n",
       "2923305  P7g9LAj9XJlWSyP5KFqY0Q  ZQ-hVBie3ls9u951cJkgSQ   \n",
       "\n",
       "                    business_id  stars  useful  funny  cool  \\\n",
       "2923301  owrGcj5a7W0QnWA7HwJ2Ag    2.0     6.0    3.0   4.0   \n",
       "2923302  -dwqsjGDBnzzv2qQYyIC3g    5.0     1.0    0.0   0.0   \n",
       "2923303                     NaN    NaN     NaN    NaN   NaN   \n",
       "2923304                     NaN    NaN     NaN    NaN   NaN   \n",
       "2923305  3nuDjIVfclE99gf4KBidKw    3.0     0.0    0.0   0.0   \n",
       "\n",
       "                                                      text  \\\n",
       "2923301  UPDATE: this store is no longer open 24/7.\\n\\n...   \n",
       "2923302  I noticed this place on the way to work one mo...   \n",
       "2923303                                                NaN   \n",
       "2923304                                                NaN   \n",
       "2923305  It's a Tampa thing...\\n\\nI was at Westshore Pl...   \n",
       "\n",
       "                        date  \n",
       "2923301  2015-06-27 03:01:22  \n",
       "2923302                  NaN  \n",
       "2923303                  NaN  \n",
       "2923304                  NaN  \n",
       "2923305  2018-01-12 22:14:25  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[2923301:2923306]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40fa7220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/01 16:23:36 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:67)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:484)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "23/01/01 16:23:36 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.nio.channels.UnresolvedAddressException\n",
      "\tat java.base/sun.nio.ch.Net.checkAddress(Net.java:149)\n",
      "\tat java.base/sun.nio.ch.Net.checkAddress(Net.java:157)\n",
      "\tat java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:337)\n",
      "\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:301)\n",
      "\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n",
      "\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n",
      "\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:503)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m sc\u001b[38;5;241m.\u001b[39mstop()\n\u001b[0;32m----> 5\u001b[0m sc \u001b[38;5;241m=\u001b[39m\u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransform Spark Session\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     12\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moptions(header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  escape\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, quotes\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myelp_academic_dataset_review.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, multiLine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:197\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:282\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:402\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1579\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1581\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1584\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1585\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1589\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "sc.stop()\n",
    "sc =SparkContext()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Transform Spark Session\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.options(header=True,  escape='\"', quotes='\"', sep=',', inferSchema=True, limit=5).csv('yelp_academic_dataset_review.csv', multiLine=True)\n",
    "df.write.parquet('yelp_academic_dataset_review.parquet', compression=\"gzip\")\n",
    "end = time.time()\n",
    "print(end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4b770328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|Even though there...|\n",
      "|Please note: the ...|\n",
      "|Is it okay to say...|\n",
      "|Great staff.  Not...|\n",
      "|Nice view from ou...|\n",
      "|Is there any way ...|\n",
      "|Updating previous...|\n",
      "|This hotel has wo...|\n",
      "|Love this place! ...|\n",
      "|Large menu. Clean...|\n",
      "|Delicious crab co...|\n",
      "|Freddy's has the ...|\n",
      "|This place had fa...|\n",
      "|I really had a wo...|\n",
      "|Allegro's is grea...|\n",
      "|This place treats...|\n",
      "|Two very good exp...|\n",
      "|One of their gues...|\n",
      "|IF YOU ARE ON A S...|\n",
      "|I took an 8 year ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6432824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf2 = dd.from_pandas(df2, npartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "67787146",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf2.to_parquet('yelp_academic_dataset_review.parquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f28c1302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Unnamed: 0'] = pd.to_numeric(df2['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6b9c5158",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__cinit__() got an unexpected keyword argument 'single_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [103]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myelp_academic_dataset_review.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgzip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:2975\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2888\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2889\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2890\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2972\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2973\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2975\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parquet.py:430\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m    428\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m--> 430\u001b[0m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parquet.py:204\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mwrite_to_dataset(\n\u001b[1;32m    196\u001b[0m             table,\n\u001b[1;32m    197\u001b[0m             path_or_handle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    201\u001b[0m         )\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;66;03m# write to single output file\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyarrow/parquet/core.py:2964\u001b[0m, in \u001b[0;36mwrite_table\u001b[0;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2962\u001b[0m use_int96 \u001b[38;5;241m=\u001b[39m use_deprecated_int96_timestamps\n\u001b[1;32m   2963\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2964\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mParquetWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2965\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2966\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2967\u001b[0m \u001b[43m            \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2969\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2970\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrite_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoerce_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_page_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_page_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2973\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_truncated_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_truncated_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2974\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2975\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_deprecated_int96_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_int96\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompression_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumn_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_page_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_page_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencryption_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrite_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[1;32m   2985\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(table, row_group_size\u001b[38;5;241m=\u001b[39mrow_group_size)\n\u001b[1;32m   2986\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyarrow/parquet/core.py:966\u001b[0m, in \u001b[0;36mParquetWriter.__init__\u001b[0;34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, **options)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_collector \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata_collector\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    965\u001b[0m engine_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 966\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m \u001b[43m_parquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParquetWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43msink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_deprecated_int96_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_deprecated_int96_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_engine_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_page_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_page_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencryption_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_open \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyarrow/_parquet.pyx:1690\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetWriter.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __cinit__() got an unexpected keyword argument 'single_file'"
     ]
    }
   ],
   "source": [
    "df2.to_parquet('yelp_academic_dataset_review.parquet', compression='gzip', single_file = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a50bbe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_process_csv(name):\n",
    "    csv_name = name + '.csv'\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Transform Spark Session\") \\\n",
    "        .config(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/jars/gcs-connector-hadoop2-latest.jar\") \\\n",
    "        .getOrCreate()\n",
    "    spark._jsc.hadoopConfiguration().set('fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem')\n",
    "    df = spark.read.options(header=True,  escape='\"', quotes='\"', sep=',', inferSchema=True, limit=5).csv(csv_name, multiLine=True)\n",
    "    df.write.parquet('gs://dana-staging/', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6d224c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_if_exists(name):\n",
    "    csv_name = name + '.csv'\n",
    "    parquet_name = name + '.parquet'\n",
    "    if cwd.joinpath(csv_name).is_file()==False:\n",
    "        return '.csv file missing'\n",
    "    elif cwd.joinpath(parquet_name).is_dir()==True:\n",
    "        shutil.rmtree(cwd.joinpath(parquet_name))\n",
    "        spark_process_csv(name)\n",
    "    elif cwd.joinpath(parquet_name).is_dir()==False:\n",
    "        spark_process_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12090bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4822b5fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msc\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m      2\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\n\u001b[1;32m      3\u001b[0m conf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/jars/gcs-connector-hadoop2-latest.jar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/jars/gcs-connector-hadoop2-latest.jar\")\n",
    "conf.set('fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem')\n",
    "conf.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\n",
    "conf.set(\"google.cloud.auth.service.account.json.keyfile\", \"alpine-furnace-373410-63fc9174bc11.json\")\n",
    "conf.set(\"spark.hadoop.fs.gs.project.id\", \"alpine-furnace-373410\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "process_if_exists('yelp_academic_dataset_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "829e0772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x28c902dc0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d066637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion_to_parquet(name):\n",
    "    csv_name = name + '.csv'\n",
    "    table = pv.read_csv(csv_name)\n",
    "    pq.write_table(table, name +'.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7186c1c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "CSV parse error: Expected 10 columns, got 8: The main reason I leave this bad review is because of the banquet staff. Our waitress was very r ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# for csv in files:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     conversion_to_parquet(csv)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mconversion_to_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myelp_academic_dataset_review\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mconversion_to_parquet\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconversion_to_parquet\u001b[39m(name):\n\u001b[1;32m      2\u001b[0m     csv_name \u001b[38;5;241m=\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43mpv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     pq\u001b[38;5;241m.\u001b[39mwrite_table(table, name \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyarrow/_csv.pyx:1217\u001b[0m, in \u001b[0;36mpyarrow._csv.read_csv\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyarrow/_csv.pyx:1226\u001b[0m, in \u001b[0;36mpyarrow._csv.read_csv\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyarrow/error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyarrow/error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: CSV parse error: Expected 10 columns, got 8: The main reason I leave this bad review is because of the banquet staff. Our waitress was very r ..."
     ]
    }
   ],
   "source": [
    "# for csv in files:\n",
    "#     conversion_to_parquet(csv)\n",
    "\n",
    "conversion_to_parquet('yelp_academic_dataset_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70615b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
